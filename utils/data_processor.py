"""
æ•°æ®å¤„ç†å·¥å…·
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
# import talib  # æ›¿æ¢ä¸ºfinta
from finta import TA
from utils.logging_manager import LoggerMixin

class DataProcessor(LoggerMixin):
    """æ•°æ®å¤„ç†ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        self.scalers = {}
        self.feature_columns = ['open', 'high', 'low', 'close', 'volume']
        self.target_column = 'close'
        
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—æ•°æ®
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            æ¸…æ´—åçš„æ•°æ®æ¡†
        """
        self.logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        df_clean = df.copy()
        
        # ç§»é™¤é‡å¤è¡Œ
        df_clean = df_clean.drop_duplicates()
        
        # å¤„ç†ç¼ºå¤±å€¼
        df_clean = self._handle_missing_values(df_clean)
        
        # ç§»é™¤å¼‚å¸¸å€¼
        df_clean = self._remove_outliers(df_clean)
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        df_clean = self._ensure_data_types(df_clean)
        
        self.logger.info(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(df_clean)} è¡Œæ•°æ®")
        return df_clean
    
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ç¼ºå¤±å€¼"""
        # å¯¹äºä»·æ ¼æ•°æ®ï¼Œä½¿ç”¨å‰å‘å¡«å……
        price_columns = ['open', 'high', 'low', 'close']
        df[price_columns] = df[price_columns].fillna(method='ffill')
        
        # å¯¹äºæˆäº¤é‡ï¼Œä½¿ç”¨0å¡«å……
        if 'volume' in df.columns:
            df['volume'] = df['volume'].fillna(0)
        
        # ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        df = df.dropna()
        
        return df
    
    def _remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤å¼‚å¸¸å€¼"""
        # ä½¿ç”¨IQRæ–¹æ³•ç§»é™¤å¼‚å¸¸å€¼
        for col in self.feature_columns:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # ç§»é™¤å¼‚å¸¸å€¼
                df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
        
        return df
    
    def _ensure_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®"""
        # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
        for col in self.feature_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç¡®ä¿æ—¶é—´åˆ—ä¸ºdatetimeç±»å‹
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        return df
    
    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        
        Args:
            df: æ•°æ®æ¡†
            
        Returns:
            æ·»åŠ æŠ€æœ¯æŒ‡æ ‡åçš„æ•°æ®æ¡†
        """
        self.logger.info("ğŸ“Š æ·»åŠ æŠ€æœ¯æŒ‡æ ‡...")
        
        df_indicators = df.copy()
        
        # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in required_columns:
            if col not in df_indicators.columns:
                self.logger.warning(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")
                return df_indicators
        
        try:
            # ä½¿ç”¨fintaè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            # ç§»åŠ¨å¹³å‡çº¿
            df_indicators['sma_5'] = TA.SMA(df_indicators, 5)
            df_indicators['sma_20'] = TA.SMA(df_indicators, 20)
            df_indicators['ema_12'] = TA.EMA(df_indicators, 12)
            df_indicators['ema_26'] = TA.EMA(df_indicators, 26)
            
            # MACD
            macd_data = TA.MACD(df_indicators)
            if isinstance(macd_data, pd.DataFrame):
                df_indicators['macd'] = macd_data['MACD']
                df_indicators['macd_signal'] = macd_data['MACD_signal']
                df_indicators['macd_hist'] = macd_data['MACD_hist']
            else:
                # å¦‚æœè¿”å›çš„æ˜¯Seriesï¼Œå°è¯•åˆ†åˆ«è®¡ç®—
                df_indicators['macd'] = macd_data
            
            # RSI
            df_indicators['rsi'] = TA.RSI(df_indicators, 14)
            
            # å¸ƒæ—å¸¦
            bb_data = TA.BBANDS(df_indicators)
            if isinstance(bb_data, pd.DataFrame):
                df_indicators['bb_upper'] = bb_data['BB_UPPER']
                df_indicators['bb_middle'] = bb_data['BB_MIDDLE']
                df_indicators['bb_lower'] = bb_data['BB_LOWER']
            
            # éšæœºæŒ‡æ ‡
            stoch_data = TA.STOCH(df_indicators)
            if isinstance(stoch_data, pd.DataFrame):
                df_indicators['stoch_k'] = stoch_data['STOCH_K']
                df_indicators['stoch_d'] = stoch_data['STOCH_D']
            
            # ATR (å¹³å‡çœŸå®æ³¢å¹…)
            df_indicators['atr'] = TA.ATR(df_indicators, 14)
            
            # æˆäº¤é‡æŒ‡æ ‡
            df_indicators['obv'] = TA.OBV(df_indicators)
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            # å¦‚æœfintaè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨pandasåŸç”Ÿæ–¹æ³•ä½œä¸ºå¤‡é€‰
            self._add_fallback_indicators(df_indicators)
        
        # ä»·æ ¼å˜åŒ–ç‡
        df_indicators['price_change'] = df_indicators['close'].pct_change()
        df_indicators['price_change_5'] = df_indicators['close'].pct_change(periods=5)
        
        # æ³¢åŠ¨ç‡
        df_indicators['volatility'] = df_indicators['price_change'].rolling(window=20).std()
        
        self.logger.info("âœ… æŠ€æœ¯æŒ‡æ ‡æ·»åŠ å®Œæˆ")
        return df_indicators
    
    def _add_fallback_indicators(self, df: pd.DataFrame) -> None:
        """æ·»åŠ å¤‡é€‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆå½“fintaå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        try:
            # ç®€å•çš„ç§»åŠ¨å¹³å‡çº¿
            df['sma_5'] = df['close'].rolling(window=5).mean()
            df['sma_20'] = df['close'].rolling(window=20).mean()
            df['ema_12'] = df['close'].ewm(span=12).mean()
            df['ema_26'] = df['close'].ewm(span=26).mean()
            
            # ç®€å•çš„RSIè®¡ç®—
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))
            
            # ç®€å•çš„å¸ƒæ—å¸¦
            df['bb_middle'] = df['close'].rolling(window=20).mean()
            bb_std = df['close'].rolling(window=20).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
            
            self.logger.info("ä½¿ç”¨å¤‡é€‰æŠ€æœ¯æŒ‡æ ‡è®¡ç®—æ–¹æ³•")
            
        except Exception as e:
            self.logger.error(f"å¤‡é€‰æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¹Ÿå¤±è´¥: {e}")
    
    def normalize_data(self, df: pd.DataFrame, method: str = 'minmax') -> Tuple[pd.DataFrame, Dict]:
        """
        æ•°æ®æ ‡å‡†åŒ–
        
        Args:
            df: æ•°æ®æ¡†
            method: æ ‡å‡†åŒ–æ–¹æ³• ('minmax', 'standard', 'robust')
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®æ¡†å’Œç¼©æ”¾å™¨ä¿¡æ¯
        """
        self.logger.info(f"ğŸ“ ä½¿ç”¨ {method} æ–¹æ³•è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
        
        df_normalized = df.copy()
        scaler_info = {}
        
        # é€‰æ‹©ç¼©æ”¾å™¨
        if method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'standard':
            scaler = StandardScaler()
        elif method == 'robust':
            scaler = RobustScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}")
        
        # æ ‡å‡†åŒ–æ•°å€¼åˆ—
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        feature_columns = [col for col in numeric_columns if col != self.target_column]
        
        if len(feature_columns) > 0:
            # æ‹Ÿåˆç¼©æ”¾å™¨
            df_normalized[feature_columns] = scaler.fit_transform(df[feature_columns])
            
            # ä¿å­˜ç¼©æ”¾å™¨ä¿¡æ¯
            scaler_info = {
                'method': method,
                'scaler': scaler,
                'feature_columns': feature_columns
            }
        
        self.logger.info("âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
        return df_normalized, scaler_info
    
    def create_sequences(self, df: pd.DataFrame, sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:
        """
        åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        
        Args:
            df: æ•°æ®æ¡†
            sequence_length: åºåˆ—é•¿åº¦
            
        Returns:
            X: ç‰¹å¾åºåˆ—
            y: ç›®æ ‡åºåˆ—
        """
        self.logger.info(f"ğŸ”„ åˆ›å»ºé•¿åº¦ä¸º {sequence_length} çš„æ—¶é—´åºåˆ—...")
        
        # è·å–ç‰¹å¾åˆ—
        feature_columns = [col for col in df.columns if col != self.target_column]
        
        if len(feature_columns) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾åˆ—")
        
        X, y = [], []
        
        for i in range(sequence_length, len(df)):
            # ç‰¹å¾åºåˆ—
            X.append(df[feature_columns].iloc[i-sequence_length:i].values)
            # ç›®æ ‡å€¼
            y.append(df[self.target_column].iloc[i])
        
        X = np.array(X)
        y = np.array(y)
        
        self.logger.info(f"âœ… åˆ›å»ºäº† {len(X)} ä¸ªåºåˆ—ï¼Œç‰¹å¾å½¢çŠ¶: {X.shape}")
        return X, y
    
    def split_data(self, X: np.ndarray, y: np.ndarray, 
                   train_split: float = 0.7, 
                   validation_split: float = 0.15) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        åˆ†å‰²æ•°æ®é›†
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®
            train_split: è®­ç»ƒé›†æ¯”ä¾‹
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        """
        self.logger.info("âœ‚ï¸ åˆ†å‰²æ•°æ®é›†...")
        
        # é¦–å…ˆåˆ†å‰²å‡ºè®­ç»ƒé›†
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=(1-train_split), shuffle=False
        )
        
        # ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‰²éªŒè¯é›†å’Œæµ‹è¯•é›†
        test_split = 1 - validation_split / (1 - train_split)
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=test_split, shuffle=False
        )
        
        self.logger.info(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ - è®­ç»ƒ: {len(X_train)}, éªŒè¯: {len(X_val)}, æµ‹è¯•: {len(X_test)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def inverse_transform(self, data: np.ndarray, scaler_info: Dict) -> np.ndarray:
        """
        åå‘è½¬æ¢æ•°æ®
        
        Args:
            data: æ ‡å‡†åŒ–åçš„æ•°æ®
            scaler_info: ç¼©æ”¾å™¨ä¿¡æ¯
            
        Returns:
            åŸå§‹å°ºåº¦çš„æ•°æ®
        """
        if 'scaler' in scaler_info:
            scaler = scaler_info['scaler']
            return scaler.inverse_transform(data)
        return data 